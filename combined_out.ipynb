{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d68f630",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [9]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814d7e3",
   "metadata": {
    "id": "ihqGgWh3ilzG",
    "papermill": {
     "duration": 0.012088,
     "end_time": "2025-11-04T18:57:58.896388",
     "exception": false,
     "start_time": "2025-11-04T18:57:58.884300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# News Article Similarity Pipeline\n",
    "## Siamese Embedding + 5W1H Extraction\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This pipeline performs two major analytical tasks on news articles:\n",
    "\n",
    "### 1. **Semantic Similarity Analysis**\n",
    "Extracts and compares the first two paragraphs of news articles using:\n",
    "- **SentenceTransformer** (MiniLM) embeddings\n",
    "- **Cosine similarity** metrics\n",
    "\n",
    "### 2. **5W1H Entity Extraction**\n",
    "Run manually in terminal using\n",
    "\n",
    "`java -Xmx4g -cp \"$(echo $HOME/.stanfordnlp_resources/stanford-corenlp-4.5.7/*.jar | tr ' ' ':')\" \\\n",
    "edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n",
    "-port 9010 -timeout 500000 \\\n",
    "-annotators tokenize,ssplit,pos,lemma,ner,parse,depparse,coref \\\n",
    "-preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref \\\n",
    "-coref.algorithm neural\n",
    "`\n",
    "\n",
    "Extracts journalism's fundamental questions from article titles:\n",
    "- **Who** - People/Organizations involved\n",
    "- **What** - Events/Actions that occurred\n",
    "- **When** - Temporal information\n",
    "- **Where** - Locations/Places\n",
    "- **Why** - Reasons/Motivations\n",
    "- **How** - Methods/Processes\n",
    "\n",
    "Uses **Giveme5W1H** library with **Stanford CoreNLP Server** backend.\n",
    "---\n",
    "\n",
    "## üìö Key Libraries\n",
    "\n",
    "- **sentence-transformers**: Neural sentence embeddings\n",
    "- **trafilatura**: Web scraping & text extraction\n",
    "- **Giveme5W1H**: 5W1H entity extraction\n",
    "- **Stanford CoreNLP**: NLP backend for entity recognition\n",
    "- **scikit-learn**: Cosine similarity computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511b750",
   "metadata": {
    "id": "iYbOgef5mL2i",
    "papermill": {
     "duration": 0.010479,
     "end_time": "2025-11-04T18:57:58.917920",
     "exception": false,
     "start_time": "2025-11-04T18:57:58.907441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dependencies and Imports (Siamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad4e3fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T18:57:58.941354Z",
     "iopub.status.busy": "2025-11-04T18:57:58.941063Z",
     "iopub.status.idle": "2025-11-04T18:58:01.674045Z",
     "shell.execute_reply": "2025-11-04T18:58:01.672924Z"
    },
    "id": "HnEuHGK0p2Mp",
    "outputId": "323d97d4-c550-4db2-e909-e8e1080bae87",
    "papermill": {
     "duration": 2.746596,
     "end_time": "2025-11-04T18:58:01.675563",
     "exception": false,
     "start_time": "2025-11-04T18:57:58.928967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install sentence-transformers trafilatura readability-lxml bs4 lxml html5lib tqdm\n",
    "!pip -q install giveme5w1h geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e452c5d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:58:01.700016Z",
     "iopub.status.busy": "2025-11-04T18:58:01.699725Z",
     "iopub.status.idle": "2025-11-04T18:58:08.058387Z",
     "shell.execute_reply": "2025-11-04T18:58:08.056759Z"
    },
    "id": "HOJdPus3iYBx",
    "papermill": {
     "duration": 6.372146,
     "end_time": "2025-11-04T18:58:08.059350",
     "exception": false,
     "start_time": "2025-11-04T18:58:01.687204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "#  Import required libraries\n",
    "# ----------------------------\n",
    "import re, math, time, sys, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1dd14",
   "metadata": {
    "id": "SvIZqLaQmAVk",
    "papermill": {
     "duration": 0.011009,
     "end_time": "2025-11-04T18:58:08.082009",
     "exception": false,
     "start_time": "2025-11-04T18:58:08.071000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Helper functions\n",
    "\n",
    "> Add blockquote\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f984b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:58:08.107010Z",
     "iopub.status.busy": "2025-11-04T18:58:08.106502Z",
     "iopub.status.idle": "2025-11-04T18:58:08.118011Z",
     "shell.execute_reply": "2025-11-04T18:58:08.116884Z"
    },
    "id": "6BYGxWGLib6L",
    "papermill": {
     "duration": 0.025252,
     "end_time": "2025-11-04T18:58:08.118774",
     "exception": false,
     "start_time": "2025-11-04T18:58:08.093522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "#  Helper: URL validation\n",
    "# ----------------------------\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; NewsSimilarityBot/1.0; +https://example.org/bot)\"}\n",
    "\n",
    "def is_valid_url(u: str) -> bool:\n",
    "    \"\"\"Check if a given string is a valid HTTP/HTTPS URL.\"\"\"\n",
    "    if not isinstance(u, str) or not u.strip():\n",
    "        return False\n",
    "    p = urlparse(u.strip())\n",
    "    return p.scheme in {\"http\", \"https\"} and bool(p.netloc)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "#  Extract first two paragraphs from HTML\n",
    "# ----------------------------\n",
    "def first_two_paragraphs_from_html(html: str):\n",
    "    \"\"\"Return the first two meaningful paragraphs from a raw HTML document.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    root = soup.find(\"article\") or soup.find(\"main\") or soup\n",
    "    paras = []\n",
    "    for p in root.find_all(\"p\"):\n",
    "        txt = re.sub(r\"\\s+\", \" \", p.get_text(\" \", strip=True)).strip()\n",
    "        if len(txt) >= 40:  # ignore very short boilerplate text\n",
    "            paras.append(txt)\n",
    "        if len(paras) >= 2:\n",
    "            break\n",
    "    # fallback: if not enough paragraphs found\n",
    "    if len(paras) < 2:\n",
    "        paras = [re.sub(r\"\\s+\", \" \", p.get_text(\" \", strip=True)).strip()\n",
    "                 for p in root.find_all(\"p\") if p.get_text(strip=True)]\n",
    "        paras = [x for x in paras if x][:2]\n",
    "    p1 = paras[0] if len(paras) > 0 else None\n",
    "    p2 = paras[1] if len(paras) > 1 else None\n",
    "    return p1, p2\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "#  Fetch paragraphs (Trafilatura ‚Üí BeautifulSoup fallback)\n",
    "# ----------------------------\n",
    "def fetch_first_two_paragraphs(url: str, timeout=12):\n",
    "    \"\"\"\n",
    "    Try extracting readable text via Trafilatura first,\n",
    "    then fall back to raw HTML parsing using BeautifulSoup.\n",
    "    Returns (p1, p2, status) where status ‚àà {'ok','invalid_url','fetch_error','no_content'}.\n",
    "    \"\"\"\n",
    "    if not is_valid_url(url):\n",
    "        return None, None, \"invalid_url\"\n",
    "\n",
    "    # --- Attempt using Trafilatura ---\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url, no_ssl=True)\n",
    "        if downloaded:\n",
    "            text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n",
    "            if text:\n",
    "                # Split into blocks on blank lines\n",
    "                blocks = [b.strip() for b in re.split(r\"\\n\\s*\\n\", text) if b.strip()]\n",
    "                blocks = [b for b in blocks if len(b) >= 40]\n",
    "                p1 = blocks[0] if len(blocks) > 0 else None\n",
    "                p2 = blocks[1] if len(blocks) > 1 else None\n",
    "                if p1 or p2:\n",
    "                    return p1, p2, \"ok\"\n",
    "    except Exception:\n",
    "        pass  # fallback to HTML parsing\n",
    "\n",
    "    # --- Fallback using raw HTML ---\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        if r.ok and r.text:\n",
    "            p1, p2 = first_two_paragraphs_from_html(r.text)\n",
    "            if p1 or p2:\n",
    "                return p1, p2, \"ok\"\n",
    "            return None, None, \"no_content\"\n",
    "    except (requests.Timeout, requests.ConnectionError):\n",
    "        # Skip this URL silently if slow or unreachable\n",
    "        return None, None, \"fetch_error\"\n",
    "    except Exception:\n",
    "        return None, None, \"fetch_error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022552c",
   "metadata": {
    "id": "25Ixn5rkmG-w",
    "papermill": {
     "duration": 0.010736,
     "end_time": "2025-11-04T18:58:08.140350",
     "exception": false,
     "start_time": "2025-11-04T18:58:08.129614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Embeddings and similarity *functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a28efda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:58:08.163691Z",
     "iopub.status.busy": "2025-11-04T18:58:08.163393Z",
     "iopub.status.idle": "2025-11-04T18:58:08.803174Z",
     "shell.execute_reply": "2025-11-04T18:58:08.801622Z"
    },
    "id": "Omf41tQ-ifb8",
    "papermill": {
     "duration": 0.652784,
     "end_time": "2025-11-04T18:58:08.804132",
     "exception": false,
     "start_time": "2025-11-04T18:58:08.151348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "#  Initialize embedding model\n",
    "# ----------------------------\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed(text: str):\n",
    "    \"\"\"Return a unit-normalized embedding for text.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    v = model.encode([text.strip()], normalize_embeddings=True)[0]\n",
    "    return v\n",
    "\n",
    "def cosine_sim(v1, v2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    if v1 is None or v2 is None:\n",
    "        return np.nan\n",
    "    return float(np.dot(v1, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b77539",
   "metadata": {
    "id": "qNS13vINma0f",
    "papermill": {
     "duration": 0.010627,
     "end_time": "2025-11-04T18:58:08.826264",
     "exception": false,
     "start_time": "2025-11-04T18:58:08.815637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Load and peek dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44adb922",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T18:58:08.850777Z",
     "iopub.status.busy": "2025-11-04T18:58:08.850479Z",
     "iopub.status.idle": "2025-11-04T18:58:08.853728Z",
     "shell.execute_reply": "2025-11-04T18:58:08.852689Z"
    },
    "id": "kudjhnunYwRd",
    "outputId": "206c6880-67e6-411f-ec8e-9175635ce9de",
    "papermill": {
     "duration": 0.017255,
     "end_time": "2025-11-04T18:58:08.854714",
     "exception": false,
     "start_time": "2025-11-04T18:58:08.837459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18526453",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T18:58:08.878119Z",
     "iopub.status.busy": "2025-11-04T18:58:08.877861Z",
     "iopub.status.idle": "2025-11-04T18:58:09.154314Z",
     "shell.execute_reply": "2025-11-04T18:58:09.153013Z"
    },
    "id": "n7uc_MIEpz2T",
    "outputId": "bdf45f6a-fa5d-4477-c8a8-f84aff647688",
    "papermill": {
     "duration": 0.289531,
     "end_time": "2025-11-04T18:58:09.155221",
     "exception": false,
     "start_time": "2025-11-04T18:58:08.865690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 26555 rows from /home/ubuntu/nlp/News-Similarity-Detection/zenodo_with_url_exists.csv\n",
      "‚úÖ Loaded 26555 rows from zenodo_with_url_exists.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================================\n",
    "#  STEP 1: Load Dataset\n",
    "# ================================================================\n",
    "# df = pd.read_csv('drive/MyDrive/zenodo_with_url_exists.csv')\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# read from the same directory as combined.ipynb / combined.py\n",
    "base_dir = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "csv_path = os.path.join(base_dir, \"zenodo_with_url_exists.csv\")\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"‚ùå Dataset not found: {csv_path}\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"‚úÖ Loaded {len(df)} rows from {csv_path}\")\n",
    "# Basic info and sanity check\n",
    "print(f\"‚úÖ Loaded {len(df)} rows from zenodo_with_url_exists.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fd3703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:58:09.179320Z",
     "iopub.status.busy": "2025-11-04T18:58:09.179044Z",
     "iopub.status.idle": "2025-11-04T18:58:09.214001Z",
     "shell.execute_reply": "2025-11-04T18:58:09.212736Z"
    },
    "id": "N2L9MpZp4_pJ",
    "papermill": {
     "duration": 0.047885,
     "end_time": "2025-11-04T18:58:09.214914",
     "exception": false,
     "start_time": "2025-11-04T18:58:09.167029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content.pair_id</th>\n",
       "      <th>content.similarity</th>\n",
       "      <th>content.url1</th>\n",
       "      <th>content.url2</th>\n",
       "      <th>content.title1</th>\n",
       "      <th>content.title2</th>\n",
       "      <th>real_lang1</th>\n",
       "      <th>real_lang2</th>\n",
       "      <th>GEO</th>\n",
       "      <th>...</th>\n",
       "      <th>GEO_num</th>\n",
       "      <th>ENT_num</th>\n",
       "      <th>TIME_num</th>\n",
       "      <th>NAR_num</th>\n",
       "      <th>STYLE_num</th>\n",
       "      <th>TONE_num</th>\n",
       "      <th>FRAME_num</th>\n",
       "      <th>OVERALL_num</th>\n",
       "      <th>url1_exists</th>\n",
       "      <th>url2_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1484188922_1484188392</td>\n",
       "      <td>0.141053</td>\n",
       "      <td>https://www.marshallindependent.com/news/local...</td>\n",
       "      <td>https://www.austin360.com/entertainment/201912...</td>\n",
       "      <td>New brewery  in Sleepy Eye draws a crowd , Spo...</td>\n",
       "      <td>New North Austin brewery, Hopsquad, opens befo...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Very Dissimilar</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1484299489_1484110209</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>https://www.washingtonpost.com/local/winning-n...</td>\n",
       "      <td>https://www.washingtonpost.com/world/the_ameri...</td>\n",
       "      <td>Winning numbers drawn in ‚ÄòCash4Life‚Äô game</td>\n",
       "      <td>Haiti‚Äôs leader marks independence day amid sec...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Very Dissimilar</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1484188635_1484299664</td>\n",
       "      <td>0.097911</td>\n",
       "      <td>https://www.opednews.com/articles/Hunter-finds...</td>\n",
       "      <td>https://guardian.ng/features/health/leading-an...</td>\n",
       "      <td>Article- Hunter finds Schuylkill- The \"Hidden ...</td>\n",
       "      <td>Leading and managing organisational change</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Very Dissimilar</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1484188331_1484347237</td>\n",
       "      <td>0.129065</td>\n",
       "      <td>https://www.irishtimes.com/news/offbeat/a-case...</td>\n",
       "      <td>https://www.amazon.com/Normal-People-Novel-Sal...</td>\n",
       "      <td>A case of criminal lunacy- The sad killing of ...</td>\n",
       "      <td>Normal People: A Novel: Sally Rooney: 97819848...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Very Similar</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1484010809_1484223087</td>\n",
       "      <td>0.188465</td>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2020/...</td>\n",
       "      <td>https://wreg.com/2020/01/01/how-to-succeed-at-...</td>\n",
       "      <td>The joy audit- how to have more fun in 2020</td>\n",
       "      <td>How to succeed at your New Years resolutions t...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Very Similar</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        content.pair_id  content.similarity  \\\n",
       "0           0  1484188922_1484188392            0.141053   \n",
       "1           1  1484299489_1484110209            0.243056   \n",
       "2           2  1484188635_1484299664            0.097911   \n",
       "3           3  1484188331_1484347237            0.129065   \n",
       "4           4  1484010809_1484223087            0.188465   \n",
       "\n",
       "                                        content.url1  \\\n",
       "0  https://www.marshallindependent.com/news/local...   \n",
       "1  https://www.washingtonpost.com/local/winning-n...   \n",
       "2  https://www.opednews.com/articles/Hunter-finds...   \n",
       "3  https://www.irishtimes.com/news/offbeat/a-case...   \n",
       "4  https://www.theguardian.com/lifeandstyle/2020/...   \n",
       "\n",
       "                                        content.url2  \\\n",
       "0  https://www.austin360.com/entertainment/201912...   \n",
       "1  https://www.washingtonpost.com/world/the_ameri...   \n",
       "2  https://guardian.ng/features/health/leading-an...   \n",
       "3  https://www.amazon.com/Normal-People-Novel-Sal...   \n",
       "4  https://wreg.com/2020/01/01/how-to-succeed-at-...   \n",
       "\n",
       "                                      content.title1  \\\n",
       "0  New brewery  in Sleepy Eye draws a crowd , Spo...   \n",
       "1          Winning numbers drawn in ‚ÄòCash4Life‚Äô game   \n",
       "2  Article- Hunter finds Schuylkill- The \"Hidden ...   \n",
       "3  A case of criminal lunacy- The sad killing of ...   \n",
       "4        The joy audit- how to have more fun in 2020   \n",
       "\n",
       "                                      content.title2 real_lang1 real_lang2  \\\n",
       "0  New North Austin brewery, Hopsquad, opens befo...         en         en   \n",
       "1  Haiti‚Äôs leader marks independence day amid sec...         en         en   \n",
       "2         Leading and managing organisational change         en         en   \n",
       "3  Normal People: A Novel: Sally Rooney: 97819848...         en         en   \n",
       "4  How to succeed at your New Years resolutions t...         en         en   \n",
       "\n",
       "               GEO  ... GEO_num ENT_num TIME_num NAR_num STYLE_num TONE_num  \\\n",
       "0  Very Dissimilar  ...     1.0     2.0      4.0     4.0       4.0      4.0   \n",
       "1  Very Dissimilar  ...     1.0     1.0      4.0     1.0       4.0      1.0   \n",
       "2  Very Dissimilar  ...     1.0     1.0      4.0     1.0       1.0      1.0   \n",
       "3     Very Similar  ...     4.0     1.0      1.0     1.0       1.0      1.0   \n",
       "4     Very Similar  ...     4.0     2.0      4.0     3.0       4.0      4.0   \n",
       "\n",
       "  FRAME_num OVERALL_num  url1_exists  url2_exists  \n",
       "0       NaN         2.0         True         True  \n",
       "1       NaN         1.0        False        False  \n",
       "2       NaN         1.0         True         True  \n",
       "3       NaN         1.0         True        False  \n",
       "4       NaN         1.0         True        False  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989e26c",
   "metadata": {
    "id": "-Q1QyJPzmhJE",
    "papermill": {
     "duration": 0.011141,
     "end_time": "2025-11-04T18:58:09.238273",
     "exception": false,
     "start_time": "2025-11-04T18:58:09.227132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### New URL Columns Appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310cbe25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T18:58:09.262915Z",
     "iopub.status.busy": "2025-11-04T18:58:09.262416Z",
     "iopub.status.idle": "2025-11-04T18:58:09.267342Z",
     "shell.execute_reply": "2025-11-04T18:58:09.266341Z"
    },
    "id": "4may5-XvmWNJ",
    "outputId": "dcd05394-8743-4629-bf4c-8fab3bb32c1d",
    "papermill": {
     "duration": 0.019117,
     "end_time": "2025-11-04T18:58:09.268746",
     "exception": false,
     "start_time": "2025-11-04T18:58:09.249629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found URL columns: ['content.url1', 'content.url2']\n"
     ]
    }
   ],
   "source": [
    "url_cols = [c for c in (\"content.url1\", \"content.url2\") if c in df.columns]\n",
    "\n",
    "# Sanity check: ensure both expected columns exist\n",
    "assert len(url_cols) == 2, (\n",
    "    f\"Expected columns content.url1 & content.url2, but found: {url_cols}\"\n",
    ")\n",
    "print(f\"‚úÖ Found URL columns: {url_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d14526",
   "metadata": {
    "id": "ewZrFKAcmqV_",
    "papermill": {
     "duration": 0.011167,
     "end_time": "2025-11-04T18:58:09.291723",
     "exception": false,
     "start_time": "2025-11-04T18:58:09.280556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Extraction, Embedding and Comparison Using Siamese Model\n",
    " - Extraction of text using helper functions\n",
    " - Embedded text from each set of urls\n",
    " - Calculated cosine similarity using the embeddings\n",
    " - Results Storage and Display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167d6ef",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7dc1db9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "f17ce837de914e4e8f388dd849c21b96",
      "83277d511402433db18fd5a0a3c1c334",
      "65ebae662e9f425d8b2426753e5d03b4",
      "6fdd7b7cc424472ba584e966de4414c0",
      "de3a6ad282794af984f36bbe3e4c4265",
      "13083f862837474cbae0a0cb390e23cf",
      "0fc8e2b56efc466693c15bcc2485ced4",
      "ad45e1b6b46d4988b3e5000161d2d57a",
      "c3749d99621f44f295933f636970e673",
      "9bcbf12e1cd84999bc58c6464f5c6103",
      "71088dbc5ad24a94831ae07a42ea7de4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-11-04T18:58:09.316684Z",
     "iopub.status.busy": "2025-11-04T18:58:09.316386Z",
     "iopub.status.idle": "2025-11-04T19:02:40.654793Z",
     "shell.execute_reply": "2025-11-04T19:02:40.653296Z"
    },
    "id": "6Yhq_-dNjQ3H",
    "outputId": "767e9eeb-b1f8-46a9-fc7f-fcb46a08e37a",
    "papermill": {
     "duration": 271.352099,
     "end_time": "2025-11-04T19:02:40.655965",
     "exception": true,
     "start_time": "2025-11-04T18:58:09.303866",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3cb13f8f0244bf98b9273c37e03f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling + Encoding:   0%|          | 0/26555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- Fetch first 2 paragraphs from both URLs ---\u001b[39;00m\n\u001b[32m     10\u001b[39m p1a, p1b, s1 = fetch_first_two_paragraphs(u1) \u001b[38;5;28;01mif\u001b[39;00m is_valid_url(u1) \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33minvalid_url\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m p2a, p2b, s2 = fetch_first_two_paragraphs(u2) \u001b[38;5;28;01mif\u001b[39;00m is_valid_url(u2) \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33minvalid_url\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# --- Combine first + second paragraphs ---\u001b[39;00m\n\u001b[32m     14\u001b[39m t1 = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (p1a, p1b) \u001b[38;5;28;01mif\u001b[39;00m x])\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  STEP 2: Extract, Embed, and Compare using Siamese Model\n",
    "# ================================================================\n",
    "rows = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Crawling + Encoding\"):\n",
    "    u1, u2 = row[url_cols[0]], row[url_cols[1]]\n",
    "\n",
    "    # --- Fetch first 2 paragraphs from both URLs ---\n",
    "    p1a, p1b, s1 = fetch_first_two_paragraphs(u1) if is_valid_url(u1) else (None, None, \"invalid_url\")\n",
    "    p2a, p2b, s2 = fetch_first_two_paragraphs(u2) if is_valid_url(u2) else (None, None, \"invalid_url\")\n",
    "\n",
    "    # --- Combine first + second paragraphs ---\n",
    "    t1 = \" \".join([x for x in (p1a, p1b) if x])\n",
    "    t2 = \" \".join([x for x in (p2a, p2b) if x])\n",
    "\n",
    "    # --- Compute embeddings + cosine similarity ---\n",
    "    v1 = embed(t1)\n",
    "    v2 = embed(t2)\n",
    "    cos = cosine_sim(v1, v2)\n",
    "\n",
    "    rows.append({\n",
    "        \"content.url1\": u1,\n",
    "        \"content.url2\": u2,\n",
    "        \"url1.p1\": p1a, \"url1.p2\": p1b, \"url1.status\": s1,\n",
    "        \"url2.p1\": p2a, \"url2.p2\": p2b, \"url2.status\": s2,\n",
    "        \"siamese.text1\": t1 or None,\n",
    "        \"siamese.text2\": t2 or None,\n",
    "        \"similarity.cosine\": cos,\n",
    "    })\n",
    "\n",
    "# --- Merge results into main dataframe ---\n",
    "result = pd.DataFrame(rows)\n",
    "siamese_output = df.join(result.drop(columns=[\"content.url1\", \"content.url2\"]).set_index(result.index))\n",
    "siamese_output.to_csv(\"siamese_output.csv\", index=False)\n",
    "\n",
    "# --- Preview results instead of saving ---\n",
    "print(\"‚úÖ Siamese comparison complete.\")\n",
    "display(siamese_output.head())\n",
    "print(f\"Total processed pairs: {len(siamese_output)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb580b",
   "metadata": {
    "id": "h6OsplJesr5B",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### Final Result Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca236a4c",
   "metadata": {
    "id": "CcpPykIIskws",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "siamese_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68edc23",
   "metadata": {
    "id": "PsRLCOHJnV2k",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "731098eb",
   "metadata": {
    "id": "6T4IosABoJj4",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Dependencies and Imports (5W1H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f78a0e",
   "metadata": {
    "id": "wIdb3EE1jUu0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "from datetime import datetime, timezone\n",
    "from Giveme5W1H.extractor.extractor import MasterExtractor\n",
    "from Giveme5W1H.extractor.document import Document\n",
    "from Giveme5W1H.extractor.extractors.environment_extractor import EnvironmentExtractor\n",
    "from Giveme5W1H.extractor.preprocessors.preprocessor_core_nlp import Preprocessor\n",
    "from geopy.geocoders import options\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5246c182",
   "metadata": {
    "id": "mu5kepsQo_rF",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### Giveme5W1H extractor and CoreNLP preprocessor setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ded204",
   "metadata": {
    "id": "uVkMIMH71toZ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "###### Install Dependencies and initialize stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146f49e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "09b999b437e645e59957a4fc53ded0fc",
      "44cb1b95f7b347ecbe82659ab0b23ac5",
      "6f7d1885c147477a88e1c3e3ea32c871",
      "d57de049a9d4441ba02c73a3b60fc7b1",
      "476758f7fbd24d758752f76df3674d38",
      "5185436d306a4739b15d483aa25f4fa3",
      "26815b30f8c94340a2a401eb92eb8c7f",
      "1a66214ec9734034a47bf4b6361e8971",
      "1bf1cb85a19345be902255a56b3edab2",
      "44984ce54c134f439d167e0340323b4a",
      "0c5b55b5c370440b864c267cc27624b1"
     ]
    },
    "id": "z0gLlbRGxaiw",
    "outputId": "e36fcdbc-5af3-484a-e30b-9edb31745255",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Java and stanza\n",
    "!apt-get update -q\n",
    "!apt-get install -y openjdk-17-jdk unzip wget -q\n",
    "!pip install stanza requests -q\n",
    "\n",
    "# Download and install CoreNLP via stanza helper\n",
    "import stanza\n",
    "stanza.install_corenlp()   # This will download stanford-corenlp-4.5.5 to ~/.stanfordnlp_resources/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce29124",
   "metadata": {
    "id": "dzyogJYvxQB4",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from stanza.server import CoreNLPClient\n",
    "\n",
    "# with CoreNLPClient(\n",
    "#     annotators=['tokenize','ssplit','pos','lemma','ner','parse','depparse'],\n",
    "#     timeout=60000,\n",
    "#     memory='2G',\n",
    "#     be_quiet=False\n",
    "# ) as client:\n",
    "#     ann = client.annotate(\"Barack Obama was born in Hawaii.\")\n",
    "#     print(ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239bf6a",
   "metadata": {
    "id": "FnwnX1Za13LR",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "###### Connect to preprocessor server and configure Giveme5W1H extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f688e4a",
   "metadata": {
    "id": "ptCPUbiB3Gb8",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_setup_corenlp():\n",
    "    \"\"\"Download and extract Stanford CoreNLP.\"\"\"\n",
    "    base_path = os.path.expanduser(\"~/.stanfordnlp_resources\")\n",
    "    corenlp_version = \"4.5.7\"\n",
    "    corenlp_dir = f\"{base_path}/stanford-corenlp-{corenlp_version}\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(corenlp_dir):\n",
    "        print(f\"‚úì CoreNLP already exists at {corenlp_dir}\")\n",
    "        return corenlp_dir\n",
    "\n",
    "    # Download CoreNLP\n",
    "    url = f\"https://nlp.stanford.edu/software/stanford-corenlp-{corenlp_version}.zip\"\n",
    "    zip_path = f\"{base_path}/stanford-corenlp-{corenlp_version}.zip\"\n",
    "    print(f\"‚¨áÔ∏è  Downloading CoreNLP {corenlp_version} (~500 MB)...\")\n",
    "\n",
    "    result = subprocess.run([\"wget\", \"-q\", \"--show-progress\", url, \"-O\", zip_path])\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Failed to download CoreNLP from {url}\")\n",
    "\n",
    "    print(\"üì¶ Extracting CoreNLP...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(base_path)\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"‚úì CoreNLP installed at {corenlp_dir}\")\n",
    "    return corenlp_dir\n",
    "\n",
    "\n",
    "def start_corenlp_server(corenlp_home, port=9020):\n",
    "    \"\"\"Start CoreNLP server in background.\"\"\"\n",
    "    try:\n",
    "        requests.get(f\"http://127.0.0.1:{port}\", timeout=2)\n",
    "        print(f\"‚úì CoreNLP server already running on port {port}\")\n",
    "        return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"üöÄ Starting CoreNLP server on port {port}...\")\n",
    "    cmd = [\n",
    "          \"java\", \"-Xmx4g\",\n",
    "          \"-cp\", f\"{corenlp_home}/*\",\n",
    "          \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\",\n",
    "          \"--port\", str(port),\n",
    "          \"--timeout\", \"500000\",\n",
    "          \"--annotators\", \"tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\",\n",
    "          \"--preload\", \"tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\",\n",
    "          \"--coref.algorithm\", \"neural\"\n",
    "      ]\n",
    "    process = subprocess.Popen(cmd, cwd=corenlp_home,\n",
    "                               stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "    print(\"‚è≥ Waiting for server to start\", end=\"\")\n",
    "    for i in range(90):\n",
    "        try:\n",
    "            requests.get(f\"http://127.0.0.1:{port}\", timeout=2)\n",
    "            print(f\"\\n‚úì CoreNLP server started successfully (took {i+1}s)\")\n",
    "            return\n",
    "        except Exception:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            time.sleep(1)\n",
    "    raise RuntimeError(\"‚ùå Failed to start CoreNLP server after 90 s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe30b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWQkhnmz5y3m",
    "outputId": "8b3d9e9b-a14a-4389-eb11-97f6af5a5d88",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Giveme5W1H extractor and CoreNLP preprocessor\n",
    "corenlp_home = download_and_setup_corenlp()\n",
    "# start_corenlp_server(corenlp_home, port=9020) only if not manually ran\n",
    "options.default_user_agent = \"colab-giveme5w1h\"\n",
    "try:\n",
    "  pre = Preprocessor(\"http://127.0.0.1:9020\")  # assumes CoreNLP server is running\n",
    "  pre._Preprocessor__default_annotators = \"tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\"\n",
    "\n",
    "  print(\"‚úÖ CoreNLP server is running!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå CoreNLP server not reachable:\", e)\n",
    "\n",
    "extractor = MasterExtractor(preprocessor=pre)\n",
    "\n",
    "# remove environment extractor for speed\n",
    "extractor.extractors = [e for e in extractor.extractors if not isinstance(e, EnvironmentExtractor)]\n",
    "\n",
    "TITLE_COLS = [c for c in (\"content.title1\", \"content.title2\") if c in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e099cb9",
   "metadata": {
    "id": "YtOMwQzxpOlf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Helper function to extract 5w1h from title and process it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864629f2",
   "metadata": {
    "id": "5R8Djr-0Gr_5",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "###### Sanitize Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d32923",
   "metadata": {
    "id": "4Olx_6i2GvTI",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import html, re, json\n",
    "\n",
    "def sanitize_title(title: str) -> str:\n",
    "    \"\"\"Clean up titles for better NLP parsing.\"\"\"\n",
    "    title = re.sub(r'\\s+', ' ', title)\n",
    "    title = re.sub(r'[^\\w\\s,\\'\"-]', '', title)\n",
    "    return title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90279936",
   "metadata": {
    "id": "xWTXwvI9jbOl",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ner_fallback(text: str):\n",
    "    \"\"\"Fallback extractor using CoreNLP NER for short texts.\"\"\"\n",
    "    props = {\"annotators\": \"tokenize,ssplit,pos,lemma,ner\", \"outputFormat\": \"json\"}\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"http://127.0.0.1:9020\",\n",
    "            params={\"properties\": json.dumps(props)},\n",
    "            data=text.encode(\"utf-8\"),\n",
    "            headers={\"Content-Type\": \"text/plain; charset=utf-8\"},\n",
    "            timeout=15\n",
    "        )\n",
    "        data = r.json()\n",
    "        entities = [(ent[\"text\"], ent[\"ner\"])\n",
    "                    for sent in data.get(\"sentences\", [])\n",
    "                    for ent in sent.get(\"entitymentions\", [])]\n",
    "\n",
    "        who = \" \".join([t for t, n in entities if n in {\"PERSON\", \"ORGANIZATION\"}]) or None\n",
    "        where = \" \".join([t for t, n in entities if n in {\"LOCATION\", \"CITY\", \"STATE_OR_PROVINCE\", \"COUNTRY\"}]) or None\n",
    "        when = \" \".join([t for t, n in entities if n in {\"DATE\", \"TIME\"}]) or None\n",
    "\n",
    "        what = re.sub(r\"[^\\w\\s,'-]\", \"\", text).strip() or None\n",
    "        return {\"who\": who, \"what\": what, \"when\": when, \"where\": where, \"why\": None, \"how\": None}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå NER fallback failed: {e}\")\n",
    "        return {k: None for k in [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"]}\n",
    "\n",
    "\n",
    "def extract_5w1h_from_title(title: str):\n",
    "    \"\"\"Main extractor with GiveMe5W1H + fallback.\"\"\"\n",
    "    if not isinstance(title, str) or not title.strip():\n",
    "        return {k: None for k in [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"]}\n",
    "    try:\n",
    "        doc = Document.from_text(title)\n",
    "        doc = extractor.parse(doc)\n",
    "        def safe_extract(q):\n",
    "            try:\n",
    "                return doc.get_top_answer(q).get_parts_as_text()\n",
    "            except Exception:\n",
    "                return None\n",
    "        result = {q: safe_extract(q) for q in [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"]}\n",
    "        if all(v is None for v in result.values()):\n",
    "            result.update(ner_fallback(title))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fallback triggered for '{title[:60]}...': {e}\")\n",
    "        return ner_fallback(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b46c39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7KnvkCiqt9nA",
    "outputId": "3f34fd44-e916-43d4-9238-e657ec6be63d",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(extract_5w1h_from_title(\"Porter‚Äôs factor conditions for countries‚Äô competitiveness are demand condition, related industries, firm‚Äôs strategy, and the level of rivalry. Australia has been challenged for the trophy and there was an increased demand for the country to produce results in the game. The country through several failures to lift the trophy had learned its weaknesses, and in 2005, it went to the games with polished strategies to face their rivals. This led to its victory.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd7285",
   "metadata": {
    "id": "iDa5L9HHprw_",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2116ef",
   "metadata": {
    "id": "EfWFAlcLlYED",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Apply Across Titles\n",
    "# ============================================================\n",
    "def process_titles_once(df, title_cols):\n",
    "    \"\"\"Apply 5W1H extraction to all given title columns with progress tracking.\"\"\"\n",
    "    result_df = df.copy()\n",
    "    for col in title_cols:\n",
    "        if col not in result_df.columns:\n",
    "            print(f\"‚ö†Ô∏è Skipping {col}: not found in dataframe\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüöÄ Processing {col} ({len(result_df)} titles)\")\n",
    "        results = []\n",
    "        for idx, title in enumerate(result_df[col].fillna(\"\")):\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"  ‚Üí Progress: {idx}/{len(result_df)}\", end=\"\\r\")\n",
    "            if idx % 100 == 0 and idx > 0:\n",
    "                time.sleep(0.3)\n",
    "\n",
    "            clean_title = re.sub(r\"\\s+\", \" \", str(title)).strip()\n",
    "            if clean_title:\n",
    "                results.append(extract_5w1h_from_title(clean_title))\n",
    "            else:\n",
    "                results.append({k: None for k in [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"]})\n",
    "\n",
    "        col_results = pd.DataFrame.from_records(results, index=result_df.index)\n",
    "        for q in col_results.columns:\n",
    "            result_df[f\"{col}.{q}\"] = col_results[q].values\n",
    "        print(f\"‚úÖ Completed {col} ({len(result_df)} titles)\")\n",
    "\n",
    "    result_df = result_df.loc[:, ~result_df.columns.duplicated(keep=\"last\")]\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1c4f5",
   "metadata": {
    "id": "7jQUm99cjc_x",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting 5W1H extraction...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TITLE_COLS = [c for c in (\"content.title1\", \"content.title2\") if c in df.columns]\n",
    "result_df = process_titles_once(df, TITLE_COLS)\n",
    "\n",
    "preview_cols = [f\"{c}.{q}\" for c in TITLE_COLS for q in (\"who\",\"what\",\"when\",\"where\",\"why\",\"how\")]\n",
    "keep = [c for c in [\"content.url1\",\"content.url2\",\"content.title1\",\"content.title2\"] if c in result_df.columns] + [c for c in preview_cols if c in result_df.columns]\n",
    "result_df = result_df[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee659174",
   "metadata": {
    "id": "i7mJh1nmIExa",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Testing the connection\n",
    "# import requests, json\n",
    "# text = sanitize_title(\"NASA launches Artemis mission to the Moon\")\n",
    "# props = {\n",
    "#     \"annotators\": \"tokenize,ssplit,pos,lemma,ner\",\n",
    "#     \"outputFormat\": \"json\"\n",
    "# }\n",
    "# r = requests.post(\n",
    "#     \"http://127.0.0.1:9010\",\n",
    "#     params={\"properties\": json.dumps(props)},\n",
    "#     data=text.encode(\"utf-8\"),\n",
    "#     headers={\"Content-Type\": \"text/plain; charset=utf-8\"}\n",
    "# )\n",
    "# print(r.status_code)\n",
    "# print(r.text[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f7c01",
   "metadata": {
    "id": "DbKBSKZ9pukc",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### Save and Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6638a07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "SGzgG3jepmxE",
    "outputId": "d5917868-e7bd-465a-80c8-ddf7c9314ce2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(result_df.head())\n",
    "fivew1h_output = result_df\n",
    "fivew1h_output.to_csv(\"fivew1h_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e12bd0",
   "metadata": {
    "id": "tVlFvF0C6H97",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### Entity Based Similarity & Fusion with Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683dcb8",
   "metadata": {
    "id": "XkKgUUFp6UKS",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63940b03",
   "metadata": {
    "id": "0MKFL-7E6XK4",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the previous 5W1H output directly (not from CSV)\n",
    "# fivew1h_output already has content.title1.*, content.title2.* columns\n",
    "fusion_df = fivew1h_output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322e88e",
   "metadata": {
    "id": "FpFd1Ap66c_7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper functions ---\n",
    "def calculate_entity_overlap(entity1, entity2):\n",
    "    \"\"\"Calculate overlap between two entities (Jaccard similarity).\"\"\"\n",
    "    if not entity1 or not entity2:\n",
    "        return 0.0\n",
    "\n",
    "    set1 = set(str(entity1).lower().split())\n",
    "    set2 = set(str(entity2).lower().split())\n",
    "\n",
    "    if len(set1) == 0 and len(set2) == 0:\n",
    "        return 1.0\n",
    "    if len(set1) == 0 or len(set2) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def calculate_5w1h_similarity(row):\n",
    "    \"\"\"Calculate entity-based similarity for a pair of articles.\"\"\"\n",
    "    questions = ['who', 'what', 'when', 'where', 'why', 'how']\n",
    "    similarities = {}\n",
    "\n",
    "    for q in questions:\n",
    "        col1 = f'content.title1.{q}'\n",
    "        col2 = f'content.title2.{q}'\n",
    "\n",
    "        if col1 in row.index and col2 in row.index:\n",
    "            entity1 = row[col1]\n",
    "            entity2 = row[col2]\n",
    "            similarities[q] = calculate_entity_overlap(entity1, entity2)\n",
    "        else:\n",
    "            similarities[q] = 0.0\n",
    "\n",
    "    avg_entity_sim = np.mean(list(similarities.values()))\n",
    "    return pd.Series({\n",
    "        'entity_who_sim': similarities['who'],\n",
    "        'entity_what_sim': similarities['what'],\n",
    "        'entity_when_sim': similarities['when'],\n",
    "        'entity_where_sim': similarities['where'],\n",
    "        'entity_why_sim': similarities['why'],\n",
    "        'entity_how_sim': similarities['how'],\n",
    "        'avg_entity_similarity': avg_entity_sim\n",
    "    })\n",
    "\n",
    "# --- Apply entity similarity computation ---\n",
    "entity_similarities = fusion_df.apply(calculate_5w1h_similarity, axis=1)\n",
    "fusion_df = pd.concat([fusion_df, entity_similarities], axis=1)\n",
    "\n",
    "print(\"‚úì Entity-based similarities calculated successfully.\")\n",
    "\n",
    "# Handle duplicated or non-standard column safely\n",
    "if 'avg_entity_similarity' in fusion_df.columns:\n",
    "    # If there are duplicate columns, keep only the last one\n",
    "    if isinstance(fusion_df['avg_entity_similarity'], pd.DataFrame):\n",
    "        avg_sim_col = fusion_df['avg_entity_similarity'].iloc[:, -1]\n",
    "    else:\n",
    "        avg_sim_col = fusion_df['avg_entity_similarity']\n",
    "\n",
    "    # Ensure Series is flat numeric\n",
    "    avg_sim_flat = pd.to_numeric(avg_sim_col.astype(str).str.extract(r'([\\d.]+)')[0], errors='coerce')\n",
    "    mean_val = float(avg_sim_flat.mean()) if not avg_sim_flat.isna().all() else 0.0\n",
    "    print(f\"  - Average entity similarity across dataset: {mean_val:.4f}\")\n",
    "else:\n",
    "    print(\"  - Warning: avg_entity_similarity column not found.\")\n",
    "# --- Preview output ---\n",
    "preview_cols = [\n",
    "    \"content.url1\", \"content.url2\",\n",
    "    \"content.title1\", \"content.title2\",\n",
    "    \"avg_entity_similarity\", \"entity_who_sim\", \"entity_what_sim\", \"entity_when_sim\",\n",
    "    \"entity_where_sim\", \"entity_why_sim\", \"entity_how_sim\"\n",
    "]\n",
    "print(\"\\nSample output:\")\n",
    "display(fusion_df[preview_cols].head())\n",
    "\n",
    "# For later fusion with Siamese results\n",
    "entity_output = fusion_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d38d9",
   "metadata": {
    "id": "iHnjaAvz6uKe",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "###### STEP 5: Fusion Layer (Entity + Embedding Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cab0f0",
   "metadata": {
    "id": "OYjx0Utp6tFT",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  STEP 5: Fusion Layer ‚Äî Combine Entity and Siamese Similarities\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Fusion Layer (Entity + Siamese Network)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Merge Siamese and Entity-based results ---\n",
    "fusion_df = pd.merge(\n",
    "    siamese_output,\n",
    "    entity_output[[\"content.url1\", \"content.url2\", \"avg_entity_similarity\"]],\n",
    "    on=[\"content.url1\", \"content.url2\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- Rename for clarity ---\n",
    "fusion_df = fusion_df.loc[:, ~fusion_df.columns.duplicated()]  # remove duplicates\n",
    "fusion_df.rename(columns={\"similarity.cosine\": \"siamese_similarity\"}, inplace=True)\n",
    "\n",
    "# --- Ensure numeric columns ---\n",
    "fusion_df[\"avg_entity_similarity\"] = pd.to_numeric(\n",
    "    fusion_df[\"avg_entity_similarity\"], errors=\"coerce\"\n",
    ").fillna(0)\n",
    "fusion_df[\"siamese_similarity\"] = pd.to_numeric(\n",
    "    fusion_df[\"siamese_similarity\"], errors=\"coerce\"\n",
    ").fillna(0)\n",
    "\n",
    "# --- Define fusion function ---\n",
    "def fusion_similarity(entity_sim, siamese_sim, entity_weight=0.4, siamese_weight=0.6):\n",
    "    \"\"\"Combine entity-based and embedding-based (Siamese) similarities.\"\"\"\n",
    "    return (entity_weight * entity_sim) + (siamese_weight * siamese_sim)\n",
    "\n",
    "# --- Compute fusion vectorized (faster & safer than .apply) ---\n",
    "fusion_df[\"fused_similarity\"] = fusion_similarity(\n",
    "    fusion_df[\"avg_entity_similarity\"],\n",
    "    fusion_df[\"siamese_similarity\"],\n",
    "    entity_weight=0.4,\n",
    "    siamese_weight=0.6,\n",
    ")\n",
    "\n",
    "# --- Summary statistics ---\n",
    "mean_val = fusion_df[\"fused_similarity\"].mean()\n",
    "print(\"‚úì Fusion similarity calculated successfully.\")\n",
    "print(f\"  - Average fused similarity: {mean_val:.4f}\")\n",
    "print(f\"  - Min fused similarity: {fusion_df['fused_similarity'].min():.4f}\")\n",
    "print(f\"  - Max fused similarity: {fusion_df['fused_similarity'].max():.4f}\")\n",
    "\n",
    "# --- Preview ---\n",
    "display(\n",
    "    fusion_df[\n",
    "        [\n",
    "            \"content.url1\",\n",
    "            \"content.url2\",\n",
    "            \"siamese_similarity\",\n",
    "            \"avg_entity_similarity\",\n",
    "            \"fused_similarity\",\n",
    "        ]\n",
    "    ].head()\n",
    ")\n",
    "\n",
    "# For final saving\n",
    "final_output = fusion_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed565e",
   "metadata": {
    "id": "Oe2TlxZD64aV",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#####  STEP 7: Pearson Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ea492",
   "metadata": {
    "id": "tWSfv3ys639R",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  STEP 7: Pearson Correlation Analysis\n",
    "# ================================================================\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 7: Pearson Correlation Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the latest unified dataframe from Step 5\n",
    "correlation_df = final_output.copy()\n",
    "\n",
    "# Identify available columns\n",
    "correlation_cols = [\n",
    "    \"entity_who_sim\", \"entity_what_sim\", \"entity_when_sim\",\n",
    "    \"entity_where_sim\", \"entity_why_sim\", \"entity_how_sim\",\n",
    "    \"avg_entity_similarity\", \"siamese_similarity\", \"fused_similarity\",\n",
    "]\n",
    "correlation_cols = [col for col in correlation_cols if col in correlation_df.columns]\n",
    "\n",
    "# --- Compute correlation matrix ---\n",
    "if correlation_cols:\n",
    "    correlation_matrix = correlation_df[correlation_cols].corr(method=\"pearson\")\n",
    "    print(\"\\nPearson Correlation Matrix:\")\n",
    "    print(correlation_matrix.round(3))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid similarity columns found for correlation analysis.\")\n",
    "\n",
    "# --- Key correlations ---\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Key Correlations:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def safe_corr(x, y):\n",
    "    \"\"\"Compute Pearson correlation safely.\"\"\"\n",
    "    try:\n",
    "        corr, p_val = pearsonr(\n",
    "            correlation_df[x].fillna(0).astype(float),\n",
    "            correlation_df[y].fillna(0).astype(float),\n",
    "        )\n",
    "        return corr, p_val\n",
    "    except Exception:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "if \"avg_entity_similarity\" in correlation_df.columns and \"siamese_similarity\" in correlation_df.columns:\n",
    "    corr, p_val = safe_corr(\"avg_entity_similarity\", \"siamese_similarity\")\n",
    "    print(f\"Entity vs Siamese Similarity: r={corr:.4f}, p={p_val:.4e}\")\n",
    "\n",
    "if \"fused_similarity\" in correlation_df.columns and \"siamese_similarity\" in correlation_df.columns:\n",
    "    corr, p_val = safe_corr(\"fused_similarity\", \"siamese_similarity\")\n",
    "    print(f\"Fused vs Siamese Similarity: r={corr:.4f}, p={p_val:.4e}\")\n",
    "\n",
    "if \"fused_similarity\" in correlation_df.columns and \"avg_entity_similarity\" in correlation_df.columns:\n",
    "    corr, p_val = safe_corr(\"fused_similarity\", \"avg_entity_similarity\")\n",
    "    print(f\"Fused vs Entity Similarity: r={corr:.4f}, p={p_val:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9efa32",
   "metadata": {
    "id": "-zqIQ--w7B1W",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#####  STEP 8: Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6028cf",
   "metadata": {
    "id": "nI3i6dKnQ-Lj",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = final_output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2a827",
   "metadata": {
    "id": "Xo4oSunc7EiF",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: Saving Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_columns = [\n",
    "    'content.url1', 'content.url2',\n",
    "    'content.title1', 'content.title2',\n",
    "    'content.title1.who', 'content.title1.what', 'content.title1.when',\n",
    "    'content.title1.where', 'content.title1.why', 'content.title1.how',\n",
    "    'content.title2.who', 'content.title2.what', 'content.title2.when',\n",
    "    'content.title2.where', 'content.title2.why', 'content.title2.how',\n",
    "    'entity_who_sim', 'entity_what_sim', 'entity_when_sim',\n",
    "    'entity_where_sim', 'entity_why_sim', 'entity_how_sim',\n",
    "    'avg_entity_similarity', 'siamese_similarity', 'fused_similarity'\n",
    "]\n",
    "\n",
    "final_columns = [c for c in final_columns if c in final_df.columns]\n",
    "\n",
    "# --- save results ---\n",
    "output_file = \"final_similarity_results.csv\"\n",
    "final_df[final_columns].to_csv(output_file, index=False)\n",
    "print(f\"‚úì Saved complete results to: {output_file}\")\n",
    "\n",
    "# --- save correlation matrix ---\n",
    "correlation_output = \"correlation_matrix.csv\"\n",
    "correlation_matrix.to_csv(correlation_output)\n",
    "print(f\"‚úì Saved correlation matrix to: {correlation_output}\")\n",
    "\n",
    "# --- save summary statistics (only existing numeric cols) ---\n",
    "summary_cols = [c for c in [\"avg_entity_similarity\", \"siamese_similarity\", \"fused_similarity\"]\n",
    "                if c in final_df.columns]\n",
    "if summary_cols:\n",
    "    summary_stats = final_df[summary_cols].describe()\n",
    "    summary_output = \"similarity_statistics.csv\"\n",
    "    summary_stats.to_csv(summary_output)\n",
    "    print(f\"‚úì Saved summary statistics to: {summary_output}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No similarity columns found for summary statistics.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74cb44",
   "metadata": {
    "id": "xcPa6xJl7L1U",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#####  STEP 9: Visualization and Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1708953",
   "metadata": {
    "id": "lcQf1fX-7J4r",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 9: Summary Statistics & Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSimilarity Score Distribution:\")\n",
    "print(final_df[correlation_cols].describe().round(4))\n",
    "\n",
    "# Find top similar pairs\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Top 10 Most Similar Article Pairs (by Fused Similarity):\")\n",
    "print(\"-\"*60)\n",
    "top_similar = final_df.nlargest(10, 'fused_similarity')[\n",
    "    ['content.title1', 'content.title2', 'avg_entity_similarity',\n",
    "     'siamese_similarity', 'fused_similarity']\n",
    "]\n",
    "print(top_similar.to_string(index=False))\n",
    "\n",
    "# Find least similar pairs\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Top 10 Least Similar Article Pairs (by Fused Similarity):\")\n",
    "print(\"-\"*60)\n",
    "least_similar = final_df.nsmallest(10, 'fused_similarity')[\n",
    "    ['content.title1', 'content.title2', 'avg_entity_similarity',\n",
    "     'siamese_similarity', 'fused_similarity']\n",
    "]\n",
    "print(least_similar.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Analysis Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  1. {output_file}\")\n",
    "print(f\"  2. {correlation_output}\")\n",
    "print(f\"  3. {summary_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa837a5",
   "metadata": {
    "id": "gyNxqcdQ7q-6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#####  STEP 10: Quantitative Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf119e5",
   "metadata": {
    "id": "r-MQBF5C7nFd",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_curve, roc_curve, auc,\n",
    "    f1_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 10: Quantitative Evaluation Metrics\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063a1c9",
   "metadata": {
    "id": "mvQRLM9E8bYU",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  Convert Continuous Scores to Binary Predictions\n",
    "# ================================================================\n",
    "\n",
    "def evaluate_at_threshold(y_true, y_scores, threshold):\n",
    "    \"\"\"Evaluate predictions at a specific threshold.\"\"\"\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2817cd",
   "metadata": {
    "id": "X-Nf8lVx8nNf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  STEP 10: Use Real Ground Truth Labels\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 10: Using Real Ground Truth Labels (content.similarity)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if \"content.similarity\" in final_df.columns:\n",
    "    # Use 0.5 as threshold to convert similarity into binary labels\n",
    "    final_df[\"ground_truth\"] = (final_df[\"content.similarity\"] >= 0.5).astype(int)\n",
    "    print(\"‚úÖ Ground truth labels loaded from 'content.similarity'.\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå 'content.similarity' column not found in dataset.\")\n",
    "\n",
    "print(f\"Total labelled pairs: {len(final_df)}\")\n",
    "print(final_df[[\"content.similarity\", \"fused_similarity\", \"ground_truth\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c0062a",
   "metadata": {
    "id": "vW_yVavf8fCz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Test multiple thresholds\n",
    "thresholds_to_test = np.arange(0.1, 1.0, 0.05)\n",
    "y_true = final_df['ground_truth'].values\n",
    "\n",
    "# Evaluate each similarity measure\n",
    "similarity_measures = {\n",
    "    'Entity Similarity': 'avg_entity_similarity',\n",
    "    'Siamese Similarity': 'siamese_similarity',\n",
    "    'Fused Similarity': 'fused_similarity'\n",
    "}\n",
    "\n",
    "results_by_measure = {}\n",
    "\n",
    "for measure_name, measure_col in similarity_measures.items():\n",
    "    if measure_col in final_df.columns:\n",
    "        y_scores = final_df[measure_col].fillna(0).values\n",
    "\n",
    "        # Evaluate at different thresholds\n",
    "        threshold_results = []\n",
    "        for thresh in thresholds_to_test:\n",
    "            metrics = evaluate_at_threshold(y_true, y_scores, thresh)\n",
    "            threshold_results.append(metrics)\n",
    "\n",
    "        results_by_measure[measure_name] = {\n",
    "            'scores': y_scores,\n",
    "            'threshold_results': pd.DataFrame(threshold_results)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722a52e",
   "metadata": {
    "id": "IGV5FtlO8qns",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Optimal Thresholds (Maximum F1 Score):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "optimal_thresholds = {}\n",
    "\n",
    "for measure_name, data in results_by_measure.items():\n",
    "    df_thresh = data['threshold_results']\n",
    "    optimal_idx = df_thresh['f1'].idxmax()\n",
    "    optimal_row = df_thresh.iloc[optimal_idx]\n",
    "    optimal_thresholds[measure_name] = optimal_row['threshold']\n",
    "\n",
    "    print(f\"\\n{measure_name}:\")\n",
    "    print(f\"  Optimal Threshold: {optimal_row['threshold']:.3f}\")\n",
    "    print(f\"  Accuracy:  {optimal_row['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {optimal_row['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {optimal_row['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {optimal_row['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c8719e",
   "metadata": {
    "id": "a0C1cSeH7wOy",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "######   Confusion Matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7f88b",
   "metadata": {
    "id": "TzeVP-Gc78xl",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating Confusion Matrices...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Confusion Matrices at Optimal Thresholds', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (measure_name, measure_col) in enumerate(similarity_measures.items()):\n",
    "    if measure_col in final_df.columns:\n",
    "        y_scores = final_df[measure_col].fillna(0).values\n",
    "        threshold = optimal_thresholds[measure_name]\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # Plot\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   cbar=False, ax=axes[idx],\n",
    "                   xticklabels=['Not Similar', 'Similar'],\n",
    "                   yticklabels=['Not Similar', 'Similar'])\n",
    "        axes[idx].set_title(f'{measure_name}\\n(Threshold: {threshold:.3f})')\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: confusion_matrices.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47362ded",
   "metadata": {
    "id": "YtIcwCeJ8w4_",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "#  Precision-Recall Curves\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating Precision-Recall Curves...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for measure_name, data in results_by_measure.items():\n",
    "    y_scores = data['scores']\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    ax.plot(recall, precision, label=f'{measure_name} (AUC={pr_auc:.3f})', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('precision_recall_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: precision_recall_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c40d4",
   "metadata": {
    "id": "rg5nFDOj82N2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "#  ROC Curves\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating ROC Curves...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for measure_name, data in results_by_measure.items():\n",
    "    y_scores = data['scores']\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    ax.plot(fpr, tpr, label=f'{measure_name} (AUC={roc_auc:.3f})', linewidth=2)\n",
    "\n",
    "# Plot diagonal line\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: roc_curves.png\")\n",
    "plt.show()\n",
    "\n",
    "# ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3bea54",
   "metadata": {
    "id": "uu_uAc1T87RE",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  F1 Score vs Threshold Curves\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating F1 Score vs Threshold Curves...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for measure_name, data in results_by_measure.items():\n",
    "    df_thresh = data['threshold_results']\n",
    "    ax.plot(df_thresh['threshold'], df_thresh['f1'],\n",
    "           label=measure_name, linewidth=2, marker='o', markersize=3)\n",
    "\n",
    "ax.set_xlabel('Threshold', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title('F1 Score vs Threshold', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1_vs_threshold.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: f1_vs_threshold.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a3023",
   "metadata": {
    "id": "d9bH3EAX9GWC",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  Precision, Recall, F1 vs Threshold (Combined)\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating Combined Metrics vs Threshold...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Metrics vs Threshold for Different Similarity Measures',\n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "metric_names = ['precision', 'recall', 'f1']\n",
    "metric_labels = ['Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "for idx, (metric, label) in enumerate(zip(metric_names, metric_labels)):\n",
    "    for measure_name, data in results_by_measure.items():\n",
    "        df_thresh = data['threshold_results']\n",
    "        axes[idx].plot(df_thresh['threshold'], df_thresh[metric],\n",
    "                      label=measure_name, linewidth=2, marker='o', markersize=3)\n",
    "\n",
    "    axes[idx].set_xlabel('Threshold', fontsize=12)\n",
    "    axes[idx].set_ylabel(label, fontsize=12)\n",
    "    axes[idx].set_title(f'{label} vs Threshold', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(loc='best', fontsize=9)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_xlim([0, 1])\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_vs_threshold.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: metrics_vs_threshold.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe1599",
   "metadata": {
    "id": "NeDdQFAA9MB9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Reports at Optimal Thresholds:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_eval = final_df if 'fused_similarity' in final_df.columns else result_df.copy()\n",
    "\n",
    "for measure_name, measure_col in similarity_measures.items():\n",
    "    if measure_col in df_eval.columns:\n",
    "        y_scores = df_eval[measure_col].fillna(0).values\n",
    "        threshold = optimal_thresholds[measure_name]\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "        print(f\"\\n{measure_name} (Threshold: {threshold:.3f}):\")\n",
    "        print(\"-\" * 60)\n",
    "        print(classification_report(\n",
    "            y_true, y_pred,\n",
    "            labels=[0, 1],  # <- ensures consistent output\n",
    "            target_names=['Not Similar', 'Similar'],\n",
    "            digits=4,\n",
    "            zero_division=0\n",
    "        ))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping {measure_name}: column '{measure_col}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ed5fe",
   "metadata": {
    "id": "2_ez1PWP9NNG",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparative Performance Summary:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pick correct dataframe dynamically\n",
    "df_eval = final_df if 'fused_similarity' in final_df.columns else result_df.copy()\n",
    "\n",
    "performance_data = []\n",
    "\n",
    "for measure_name, measure_col in similarity_measures.items():\n",
    "    if measure_col not in df_eval.columns:\n",
    "        print(f\"‚ö†Ô∏è Skipping '{measure_name}' ‚Äî missing column '{measure_col}'.\")\n",
    "        continue\n",
    "\n",
    "    y_scores = df_eval[measure_col].fillna(0).values\n",
    "    threshold = optimal_thresholds.get(measure_name, np.median(y_scores))\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # Compute AUC scores\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(rec, prec)\n",
    "\n",
    "    performance_data.append({\n",
    "        \"Measure\": measure_name,\n",
    "        \"Optimal Threshold\": threshold,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "        \"PR-AUC\": pr_auc\n",
    "    })\n",
    "\n",
    "# --- Build dataframe safely ---\n",
    "if len(performance_data) == 0:\n",
    "    print(\"‚ùå No valid similarity columns found. Please check final_df contents.\")\n",
    "    print(\"Available columns:\", df_eval.columns.tolist())\n",
    "else:\n",
    "    performance_df = pd.DataFrame(performance_data)\n",
    "    print(\"\\n\", performance_df.to_string(index=False))\n",
    "    performance_df.to_csv(\"performance_comparison.csv\", index=False)\n",
    "    print(\"\\n‚úì Saved: performance_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d521e",
   "metadata": {
    "id": "ofAkvFbq9WVN",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  Summary\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Findings:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_idx = performance_df[\"F1\"].idxmax()\n",
    "best_measure = performance_df.iloc[best_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best Performing Measure: {best_measure['Measure']}\")\n",
    "print(f\"  F1 Score : {best_measure['F1']:.4f}\")\n",
    "print(f\"  Accuracy : {best_measure['Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {best_measure['Precision']:.4f}\")\n",
    "print(f\"  Recall   : {best_measure['Recall']:.4f}\")\n",
    "print(f\"  ROC-AUC  : {best_measure['ROC-AUC']:.4f}\")\n",
    "print(f\"  PR-AUC   : {best_measure['PR-AUC']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 284.298674,
   "end_time": "2025-11-04T19:02:41.988334",
   "environment_variables": {},
   "exception": true,
   "input_path": "combined.ipynb",
   "output_path": "combined_out.ipynb",
   "parameters": {},
   "start_time": "2025-11-04T18:57:57.689660",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0bbd1a2b6dcc4a0c859096753e34167d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "45ed401c720548cfad91d082fd479962": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "58a6e29bc1724717b5fe4cbb90b58de3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0bbd1a2b6dcc4a0c859096753e34167d",
       "max": 26555.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_72e755a91ea94d30a1d3950b6fa15154",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "61885043ab5745b2a345c61cff921579": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bfc102d3305240da988825940a0f081f",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_45ed401c720548cfad91d082fd479962",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá4/26555‚Äá[04:30&lt;391:33:35,‚Äá53.09s/it]"
      }
     },
     "72e755a91ea94d30a1d3950b6fa15154": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "943920e460684e388d4e6c0c5902be5e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9cb29bd9984b432d84b263b743cff6a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f7ec940c07e84daa91e6659692cb0207",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_abd79319b56b4ccaa0e1033b3f97c596",
       "tabbable": null,
       "tooltip": null,
       "value": "Crawling‚Äá+‚ÄáEncoding:‚Äá‚Äá‚Äá0%"
      }
     },
     "abd79319b56b4ccaa0e1033b3f97c596": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bb3cb13f8f0244bf98b9273c37e03f38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9cb29bd9984b432d84b263b743cff6a4",
        "IPY_MODEL_58a6e29bc1724717b5fe4cbb90b58de3",
        "IPY_MODEL_61885043ab5745b2a345c61cff921579"
       ],
       "layout": "IPY_MODEL_943920e460684e388d4e6c0c5902be5e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "bfc102d3305240da988825940a0f081f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f7ec940c07e84daa91e6659692cb0207": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}